import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

from tensorflow.keras.models import Model # type: ignore
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

# === Load the dataset ===
file_path = r"C:\Users\Bryan\OneDrive\Desktop\creditcard.csv"
df = pd.read_csv(file_path)

# === Feature Scaling ===
scaler = StandardScaler()
df['scaled_amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))
df['scaled_time'] = scaler.fit_transform(df['Time'].values.reshape(-1, 1))
df.drop(['Amount', 'Time'], axis=1, inplace=True)

# Reorder columns so scaled features come last
scaled_features = ['scaled_amount', 'scaled_time']
other_features = [col for col in df.columns if col not in ['Class'] + scaled_features]
df = df[other_features + scaled_features + ['Class']]

# === Train-Test Split ===
X = df.drop('Class', axis=1)
y = df['Class']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Calculate contamination ratio for models that need it
contamination_ratio = y_train.sum() / len(y_train)

# === 1. Isolation Forest ===
iso_forest = IsolationForest(n_estimators=100, contamination=contamination_ratio, random_state=42)
iso_forest.fit(X_train)
y_pred_iso = iso_forest.predict(X_test)
y_pred_iso = np.where(y_pred_iso == -1, 1, 0)  # -1 is anomaly, 1 = fraud

# === 2. One-Class SVM ===
svm = OneClassSVM(kernel='rbf', nu=contamination_ratio, gamma='scale')
svm.fit(X_train[y_train == 0])  # train only on normal transactions
y_pred_svm = svm.predict(X_test)
y_pred_svm = np.where(y_pred_svm == -1, 1, 0)

# === 3. Autoencoder ===
input_dim = X_train.shape[1]
input_layer = Input(shape=(input_dim,))
encoder = Dense(16, activation="relu")(input_layer)
encoder = Dense(8, activation="relu")(encoder)
decoder = Dense(16, activation="relu")(encoder)
decoder = Dense(input_dim, activation="linear")(decoder)
autoencoder = Model(inputs=input_layer, outputs=decoder)
autoencoder.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')

# Train autoencoder on normal transactions only
X_train_ae = X_train[y_train == 0]
autoencoder.fit(
    X_train_ae, X_train_ae,
    epochs=10, batch_size=256,
    shuffle=True, validation_split=0.1, verbose=1
)

# Predict reconstruction error on test set
X_test_pred = autoencoder.predict(X_test)
mse = np.mean(np.power(X_test - X_test_pred, 2), axis=1)

# Threshold set as 95th percentile MSE on normal transactions in test set
threshold = np.percentile(mse[y_test == 0], 95)
y_pred_ae = (mse > threshold).astype(int)

# === Evaluation Function ===
def evaluate_model(name, y_true, y_pred):
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred)
    print(f"--- {name} ---")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1-score:  {f1:.4f}")
    print(f"ROC-AUC:   {roc_auc:.4f}\n")
    return {
        'Model': name,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'ROC-AUC': roc_auc
    }

# Evaluate all models
results = []
results.append(evaluate_model("Isolation Forest", y_test, y_pred_iso))
results.append(evaluate_model("One-Class SVM", y_test, y_pred_svm))
results.append(evaluate_model("Autoencoder", y_test, y_pred_ae))

results_df = pd.DataFrame(results).sort_values(by='ROC-AUC', ascending=False)
print("=== Model Comparison Summary ===")
print(results_df)

# === Save predictions of the best model ===
best_model_name = results_df.iloc[0]['Model']
print(f"\nBest Model: {best_model_name}")

if best_model_name == "Isolation Forest":
    best_preds = y_pred_iso
elif best_model_name == "One-Class SVM":
    best_preds = y_pred_svm
else:
    best_preds = y_pred_ae

# Prepare output DataFrame for Excel
output_df = X_test.copy()
output_df['Actual_Fraud'] = y_test.values
output_df['Predicted_Fraud'] = best_preds

output_file_path = r"C:\Users\Bryan\OneDrive\Desktop\fraud_detection_results.xlsx"
output_df.to_excel(output_file_path, index=False)
print(f"Prediction results saved to '{output_file_path}'")

# === Plot confusion matrices for all models ===
import matplotlib.pyplot as plt
import seaborn as sns

def plot_confusion_matrix(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])
    plt.title(f'Confusion Matrix - {model_name}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

plot_confusion_matrix(y_test, y_pred_iso, "Isolation Forest")
plot_confusion_matrix(y_test, y_pred_svm, "One-Class SVM")
plot_confusion_matrix(y_test, y_pred_ae, "Autoencoder")
